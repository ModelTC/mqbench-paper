How to quantize your model?
===========================


.. _Adela Documentation: https://confluence.sensetime.com/pages/viewpage.action?pageId=187345464
.. _Nart Documentation: http://compile-link.pages.gitlab.bj.sensetime.com/nart/tutorial/switch.html#advanced-usage/
.. _Dipoorlet Documentation: http://spring.sensetime.com/docs/dipoorlet/
.. _Dirichlet Documentation: http://spring.sensetime.com/docs/dirichlet/


Two concepts
------------

1. Post Training Quantization (PTQ):

   Just calibrate quantization parameters by a small-scale calibration dataset and does not update the weights by backward propogation.

2. Quantization Aware Training (QAT):

   Finetuning a trained full-precision model in a quantization aware scheme to optimize the model to a quantization-friendly one.

Three ways to quantize your model
---------------------------------

We provide three tools to quantize your model:

1. Nart Switch (Adela)

2. Dipoorlet

3. Dirichlet

From 1 to 3, the required human effort and time cost becomes higher but the achieved accuracy may become higher.

Detailed Comparison is listed below:

.. list-table::
   :widths: 5 2 16 4 8 8
   :width: 100%
   :header-rows: 1
   :align: center

   * - Tool
     - Level
     - Human Effort Cost
     - Time Cost
     - Ability
     - Flexibility
   * - Nart Switch (Adela)
     - PTQ
     - 1. Prepare a small-scale calibration dataset.
       2. Prepare a config for Nart Switch (Adela);
       3. Build quantized engine.
     - Several minutes or hours.
     - Only can convert a quantized model, can not promote furtherly.
     - Blackbox; Impossible to extend new algorithms.
   * - \+ Dipoorlet
     - PTQ
     - 1. Prepare a small-scale calibration dataset.
       2. Calibrate qparams by Dipoorlet.
       3. Build quantized engine by Nart (Adela) with the qparams generated by Dipoorlet.
     - Several minutes or hours.
     - Integrate latest SOTA Post Training algorithms; Analyze and locate the quantization-sensitive layers, then promote the accuracy furtherly.
     - Easy to extend new algorithms; Support different hardware.
   * - \+ Dirichlet
     - QAT
     - 1. Insert necessary Dirichlet's APIs to you original full-precision training code.
       2. Prepare a quantization config file for quantization aware training.
       3. Convert the PyTorch model to onnx/caffe and extract relevant qparams by Dirichlet's API.
       4. Build quantized engine by Nart (Adela) with the qparams generated by Dirichlet.
     - Depends on time for finetuning.
     - Integrate the latest SOTA Quantization aware training algorithms and optimized the origin model to a quantizaiton-friendly one.
     - Easy to extend new algorithms; Support different hardware.


Recommended Pipeline
------------------------------

We summarize a quantization pipeline to improve the efficiency of producing a quantized model:

.. image:: ../_static/images/pipeline.png


1. PATH1 (green):

  Directly follow the instructions of Adela / Nart Switch. It encapsules the official post training quantization tool provided by the hardware company. Detailed instructions see `Adela Documentation`_ or `Nart Documentation`_. If you find the quantization error is large and the accuracy degradation is unacceptable. You can turn to the PATH2 (blue). For some hardware (e.g., PPL-DSP), no official quantization tool is provided. In this case, you should start directly from PATH2.

2. PATH2 (blue):

  Use the self-developed post training quantization tool Dipoorlet to profile and locate the quantiation-sensitive layer and try more advanced algorithms. If we find the reason resulting in the sensitivity, then we can apply corresponding method to recover the accuracy. Detailed instructions see `Dipoorlet Documentation`_. If all the algorithms do not work, you need to resort to PATH3 (red).

3. PATH3 (red):
   
  Insert Dirichlet to your training code for quantization aware training. You need to prepare a quantization config to specify where and how to quantize for the deployment on target hardware. Detailed instructions see `Dirichlet Documentation`_.

Overall, we are devoted to make more quantized models can be generated from PATH1/PATH2 and less models require PATH3. Then the complexity can be reduced and the efficiency can be improved.
