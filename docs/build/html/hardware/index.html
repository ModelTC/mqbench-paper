

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Quantization Hardware &mdash; MQBench  documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme_overrides.css" type="text/css" />
  <link rel="stylesheet" href="../_static/contentui.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/contentui.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Benchmark" href="../benchmark/index.html" />
    <link rel="prev" title="Quantization Algorithm" href="../algorithm/index.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> MQBench
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../algorithm/index.html">Quantization Algorithm</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Quantization Hardware</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="#academic-setup">Academic Setup</a></li>
<li class="toctree-l2"><a class="reference internal" href="#tensorrt-setup">TensorRT Setup</a></li>
<li class="toctree-l2"><a class="reference internal" href="#snpe-setup">SNPE Setup</a></li>
<li class="toctree-l2"><a class="reference internal" href="#tvm-setup">TVM Setup</a></li>
<li class="toctree-l2"><a class="reference internal" href="#acl-setup">ACL Setup</a></li>
<li class="toctree-l2"><a class="reference internal" href="#fbgemm-setup">FBGEMM Setup</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../benchmark/index.html">Benchmark</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MQBench</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Quantization Hardware</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/hardware/index.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="quantization-hardware">
<h1>Quantization Hardware<a class="headerlink" href="#quantization-hardware" title="Permalink to this headline">¶</a></h1>
<div class="section" id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h2>
<table class="docutils align-default">
<colgroup>
<col style="width: 20%" />
<col style="width: 10%" />
<col style="width: 8%" />
<col style="width: 13%" />
<col style="width: 17%" />
<col style="width: 13%" />
<col style="width: 10%" />
<col style="width: 6%" />
<col style="width: 4%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Inference Library</p></th>
<th class="head"><p>Provider</p></th>
<th class="head"><p>HW Type</p></th>
<th class="head"><p>Hardware</p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(s\)</span> Form</p></th>
<th class="head"><p>Granularity</p></th>
<th class="head"><p>Symmetry</p></th>
<th class="head"><p>Graph</p></th>
<th class="head"><p>FBN</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Academic</p></td>
<td><p>None</p></td>
<td><p>None</p></td>
<td><p>None</p></td>
<td><p>FP32</p></td>
<td><p>Per-tensor</p></td>
<td><p>Sym.</p></td>
<td><p>1</p></td>
<td><p>No</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://github.com/NVIDIA/TensorRT">TensorRT</a></p></td>
<td><p>NVIDIA</p></td>
<td><p>GPU</p></td>
<td><p>Tesla T4/P4</p></td>
<td><p>FP32</p></td>
<td><p>Per-channel</p></td>
<td><p>Sym.</p></td>
<td><p>2</p></td>
<td><p>Yes</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://support.huaweicloud.com/intl/en-us/ti-mc-A200_3000/altasmodelling_16_043.html">ACL</a></p></td>
<td><p>HUAWEI</p></td>
<td><p>ASIC</p></td>
<td><p>Ascend310</p></td>
<td><p>FP32</p></td>
<td><p>Per-channel</p></td>
<td><p>Asym.</p></td>
<td><p>1</p></td>
<td><p>Yes</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://www.usenix.org/conference/osdi18/presentation/chen">TVM</a></p></td>
<td><p>OctoML</p></td>
<td><p>CPU</p></td>
<td><p>ARM</p></td>
<td><p>POT</p></td>
<td><p>Per-tensor</p></td>
<td><p>Sym.</p></td>
<td><p>3</p></td>
<td><p>Yes</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://developer.qualcomm.com/software/qualcomm-neural-processing-sdk">SNPE</a></p></td>
<td><p>Qualcomm</p></td>
<td><p>DSP</p></td>
<td><p>Snapdragon</p></td>
<td><p>FP32</p></td>
<td><p>Per-tensor</p></td>
<td><p>Asym.</p></td>
<td><p>3</p></td>
<td><p>Yes</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://github.com/pytorch/FBGEMM">FBGEMM</a></p></td>
<td><p>Facebook</p></td>
<td><p>CPU</p></td>
<td><p>X86</p></td>
<td><p>FP32</p></td>
<td><p>Per-channel</p></td>
<td><p>Asym.</p></td>
<td><p>3</p></td>
<td><p>Yes</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="academic-setup">
<h2>Academic Setup<a class="headerlink" href="#academic-setup" title="Permalink to this headline">¶</a></h2>
<p>In academical research, most existing work chooses the per-tensor, symmetric quantization. This quantizer design could be challenging. However, academical setting only quantizes the input and the weight of a convolutional or linear layer. Thus the computational graph is not aligned to any hardware implementations. Note that people tend to use <em>unsigned</em> quantization to quantize input activation and <em>signed</em> quantization to quantize weights. For unsigned quantization, the target integer range is <span class="math notranslate nohighlight">\([N_{min}, N_{max}]=[0, 2^t-1]\)</span>, while for signed quantization, the range becomes <span class="math notranslate nohighlight">\([N_{min}, N_{max}]=[-2^{t-1}, 2^{t-1}-1]\)</span>.
The intuition for adopting unsigned quantization is ReLU activation are non-negative, and symmetric signed quantization will waste one bit for negative parts.
In our implementation, we add a switch variable called <em>adaptive signness</em>, which can turn the integer range to <span class="math notranslate nohighlight">\([-2^{t-1}, 2^{t-1}-1]\)</span> based on data statistics. It should be noted that <em>Adaptive signness</em> is only designed for academic setting, while symmetric quantization must waste one bit for non-negative activation in real-world hardware.</p>
</div>
<div class="section" id="tensorrt-setup">
<h2>TensorRT Setup<a class="headerlink" href="#tensorrt-setup" title="Permalink to this headline">¶</a></h2>
<p><a class="reference external" href="https://github.com/NVIDIA/TensorRT">TensorRT</a> is a high-performance inference library developed by NVIDIA. The quantization scheme in TensorRT is symmetric per-channel for weights, and symmetric per-tensor for activations. The integer range is <span class="math notranslate nohighlight">\([-128, 127]\)</span>. TensorRT will quantize every layers in the network including the elemental-wise Add and Pooling besides those layers which have weights. However, per-channel quantization scheme can reduce the error for weight quantization to a certain extent. TensorRT model will be deployed on GPUs, and Int8 computation will be achieved by Tensor Cores or DP4A instructions, which are highly efficient. Typically, one GTX1080TI GPU have 45.2 peak Tops in INT8 mode.</p>
</div>
<div class="section" id="snpe-setup">
<h2>SNPE Setup<a class="headerlink" href="#snpe-setup" title="Permalink to this headline">¶</a></h2>
<p><a class="reference external" href="https://developer.qualcomm.com/software/qualcomm-neural-processing-sdk">SNPE</a> is a neural processing engine SDK developed by Qualcomm Snapdragon for model inference on Snapdragon CPU, Adreno GPU and the Hexagon DSP. SNPE supports 8bit fixed-point quantization for Hexagon DSP. Hexagon DSP is an advanced, variable instruction lenghth, Very Long Instruction Word(VLIW) processor architecture with hardware multi-threading. The quantization scheme of SPNE is asymmetric and per-tensor for weights and activations.</p>
</div>
<div class="section" id="tvm-setup">
<h2>TVM Setup<a class="headerlink" href="#tvm-setup" title="Permalink to this headline">¶</a></h2>
<p><a class="reference external" href="https://www.usenix.org/conference/osdi18/presentation/chen">TVM</a> is a deep learning compiler and can compile neural networks to a 8-bit implementation. For now, TVM supports running the whole network in symmetric per-tensor quantization scheme. One different point is, in order to accelerate the quantization affine operation, they represent the scale as power-of-two and thus can utilize the efficient shift operation to enjoy further speed up. The quantized INT8 model compiled by TVM can be deployed on GPUs, CPUs or DSPs.</p>
</div>
<div class="section" id="acl-setup">
<h2>ACL Setup<a class="headerlink" href="#acl-setup" title="Permalink to this headline">¶</a></h2>
<p><a class="reference external" href="https://support.huaweicloud.com/intl/en-us/ti-mc-A200_3000/altasmodelling_16_043.html">ACL</a> is a neural network inference software developed by HUAWEI for the hardware named Atlas. Atlas supports INT8 convolution and linear kernel so ACL quantizes the layer which has weights, such as convolution, fully-connected layer to int8 fixed point, but remains the rest part of network in FP32. The quantization scheme is symmetric per-channel for weight, and asymmetric per-tensor for activation to avoid the waste of one bit. Typically an Atlas 300 inference card have 88 Tops in INT8 mode.</p>
</div>
<div class="section" id="fbgemm-setup">
<h2>FBGEMM Setup<a class="headerlink" href="#fbgemm-setup" title="Permalink to this headline">¶</a></h2>
<p><a class="reference external" href="https://github.com/pytorch/FBGEMM">FBGEMM</a> is a inference library developed by Facebook and can deploy torch model easily. The quantization scheme is asymmetric per-channel and we quantize the whole network into int8 fixed point.</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="../benchmark/index.html" class="btn btn-neutral float-right" title="Benchmark" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="../algorithm/index.html" class="btn btn-neutral float-left" title="Quantization Algorithm" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, Sensetime.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>